# Training parameters
train_params:
  epochs: 50
  device: "cpu" # device name, values: "cpu" or "cuda:x" where 'x' is gpu index
  optimizer: "adam"
  save_every: 10
  swa_start: 30 # start epoch of stochastic weight averaging
  patience: 3 # how many epochs we want to wait after the last time the validation loss improved before breaking the training loop
  early_stopping_delta: 0

# Logger parameters
logger:
  project: "" # project name
  tags: "exp test train"
  resume: False # (boolean) whether to resume training or not
  online: True # (boolean) whether to store logs online or not
  experiment_id: "" # can be retrieved from logger dashboard, available if only resuming
  log_dir: "./logs" # where to store log data
  disabled: False # disable the comet ml

# Dataloader parameters
dataloader:
  num_workers: 2 # Allowing multi-processing
  batch_size: 64
  shuffle: True # whether to shuffle data or not
  pin_memory: False # directly load datasets as CUDA tensors

# Train dataset parameters
dataset:
  dataset: "" #  which dataset to use if there are more than one
  data_root: "./data/train" # where data resides

# Validation dataset parameters
val_dataset:
  dataset: "" #  which dataset to use if there are more than one
  data_root: "./data/val" # where data resides

# directories
directory:
  model_name: "model-name" # file name for saved model
  save: "./checkpoint"
  load: "./checkpoint/model-name-best.pt"

# model parameters
model:
  in_features: 3
  out_features: 13

# Adam parameters if using Adam optimizer
adam:
  lr: 1e-3
  betas:
    - 0.9
    - 0.999
  eps: 1e-8
  weight_decay: 0
  amsgrad: False

# RMSprop parameters if using RMSprop optimizer
rmsprop:
  lr: 1e-3
  momentum: 0
  alpha: 0.99
  eps: 1e-8
  centered: False
  weight_decay: 0

# Stochastic Weight Averaging parameters
SWA:
  anneal_strategy: "linear"
  anneal_epochs: 5
  swa_lr: 0.05